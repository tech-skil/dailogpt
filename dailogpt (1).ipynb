{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ky9MLqnRRo7u"
      },
      "outputs": [],
      "source": [
        "# Step 1: Setup Environment\n",
        "!pip install fsspec==2024.10.0\n",
        "!pip install transformers datasets torch onnx onnxruntime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UrpXL37NbRtI"
      },
      "outputs": [],
      "source": [
        "# Step 2: Import Libraries\n",
        "import os\n",
        "import json\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments, TextDataset, DataCollatorForLanguageModeling\n",
        "import torch\n",
        "from google.colab import drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KIDgo-6Ppl4z"
      },
      "outputs": [],
      "source": [
        "# Step 3: Mount Google Drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yJKGTMoVpsCg"
      },
      "outputs": [],
      "source": [
        "# Step 4: Load Dataset\n",
        "dataset_path = '/content/drive/My Drive/generated_datasets.json'  # Update with your path\n",
        "with open(dataset_path, 'r') as f:\n",
        "    data = json.load(f)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lcI1uypUpv5V"
      },
      "outputs": [],
      "source": [
        "# Convert dataset to DialoGPT format\n",
        "train_file = \"train.txt\"\n",
        "with open(train_file, \"w\") as f:\n",
        "    for convo in data[\"conversations\"]:\n",
        "        f.write(f\"{convo['user']}\\n{convo['bot']}\\n\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "k2Fx5Qvcq-0s"
      },
      "outputs": [],
      "source": [
        "# Step 5: Load DialoGPT Model and Tokenizer\n",
        "model_name = \"microsoft/DialoGPT-medium\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "CiZvX0oerCLG"
      },
      "outputs": [],
      "source": [
        "# Step 6: Preprocessing\n",
        "def create_dataset(file_path, tokenizer, block_size=512):\n",
        "    dataset = TextDataset(\n",
        "        tokenizer=tokenizer,\n",
        "        file_path=file_path,\n",
        "        block_size=block_size,\n",
        "    )\n",
        "    return dataset\n",
        "\n",
        "def create_data_collator(tokenizer):\n",
        "    return DataCollatorForLanguageModeling(\n",
        "        tokenizer=tokenizer,\n",
        "        mlm=False,\n",
        "    )\n",
        "\n",
        "train_dataset = create_dataset(train_file, tokenizer)\n",
        "data_collator = create_data_collator(tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "dIMF5Q1syLg9"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "print(torch.cuda.is_available())  # Should return True if GPU is available\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t-uA8j2Pr1fZ",
        "outputId": "9fb18dbc-9cae-427d-dc85-eaf9e0059c7d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"
          ]
        },
        {
          "data": {
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Step 7: Training\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=2,\n",
        "    save_steps=500,\n",
        "    save_total_limit=2,\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_steps=100,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s4uTzUS1sRSk"
      },
      "outputs": [],
      "source": [
        "# Step 8: Save the Fine-Tuned Model\n",
        "os.makedirs(\"onnx\", exist_ok=True)\n",
        "os.makedirs(\"pytorch\", exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BrsIuPy-1ZKG"
      },
      "outputs": [],
      "source": [
        "# Save PyTorch Model\n",
        "torch.save(model.state_dict(), \"pytorch/dialoGPT_model.pth\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z7YCGYRx1abn"
      },
      "outputs": [],
      "source": [
        "# Save ONNX Model\n",
        "dummy_input = torch.zeros(1, 1).long()\n",
        "torch.onnx.export(\n",
        "    model,\n",
        "    dummy_input,\n",
        "    \"onnx/dialoGPT_model.onnx\",\n",
        "    input_names=[\"input_ids\"],\n",
        "    output_names=[\"output\"],\n",
        "    dynamic_axes={\"input_ids\": {0: \"batch_size\", 1: \"sequence_length\"}}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5NmoCJNU14LH"
      },
      "outputs": [],
      "source": [
        "# Step 9: Save to Google Drive\n",
        "!cp -r onnx /content/drive/MyDrive/fine_tuned_models/\n",
        "!cp -r pytorch /content/drive/MyDrive/fine_tuned_models/\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PPJfK29z18vw"
      },
      "outputs": [],
      "source": [
        "# Step 10: Inference\n",
        "def chat_with_bot(user_input, model, tokenizer):\n",
        "    input_ids = tokenizer.encode(user_input + tokenizer.eos_token, return_tensors=\"pt\")\n",
        "    chat_history_ids = model.generate(\n",
        "        input_ids,\n",
        "        max_length=1000,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "        top_p=0.92,\n",
        "        top_k=50,\n",
        "        temperature=0.7,\n",
        "    )\n",
        "    response = tokenizer.decode(chat_history_ids[:, input_ids.shape[-1]:][0], skip_special_tokens=True)\n",
        "    return response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lDU2UE_e1_77"
      },
      "outputs": [],
      "source": [
        "# Example Chat\n",
        "while True:\n",
        "    user_input = input(\"User: \")\n",
        "    if user_input.lower() == \"exit\":\n",
        "        break\n",
        "    bot_response = chat_with_bot(user_input, model, tokenizer)\n",
        "    print(f\"Bot: {bot_response}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e_I1F6Jj2Dhq"
      },
      "outputs": [],
      "source": [
        "Weights & Biases: 48723499de67dac0da9115c091320aa958015969"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}